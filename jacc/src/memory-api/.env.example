# Memory API Environment Configuration
# Copy this file to .env and modify as needed

# ============================================
# Database Configuration
# ============================================
POSTGRES_USER=memory
POSTGRES_PASSWORD=memory_secret
POSTGRES_DB=memory_db
DB_PORT=5432

# ============================================
# LLM Provider Configuration
# ============================================
# Providers: openai, groq, ollama, gemini
LLM_PROVIDER=openai
LLM_API_KEY=sk-your-api-key-here
LLM_MODEL=gpt-4o-mini
# LLM_BASE_URL=  # Optional: custom base URL for LLM API

# Advanced LLM settings
LLM_MAX_CONCURRENT=32  # Maximum concurrent LLM requests
LLM_TIMEOUT=120        # LLM request timeout in seconds

# For Groq:
# LLM_PROVIDER=groq
# LLM_API_KEY=gsk_your-api-key-here
# LLM_MODEL=llama-3.3-70b-versatile

# For Ollama (no API key needed):
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2
# LLM_BASE_URL=http://host.docker.internal:11434/v1

# For Gemini:
# LLM_PROVIDER=gemini
# LLM_API_KEY=your-gemini-api-key
# LLM_MODEL=gemini-2.5-flash

# ============================================
# Embeddings Configuration
# ============================================
# Providers: local, tei
# - local: Uses sentence-transformers locally (default)
# - tei: Uses Text Embeddings Inference server
EMBEDDINGS_PROVIDER=local
EMBEDDINGS_MODEL=BAAI/bge-small-en-v1.5
# EMBEDDINGS_TEI_URL=http://localhost:8080  # For TEI server

# ============================================
# Reranker Configuration
# ============================================
# Providers: local, tei
# - local: Uses cross-encoder locally (default)
# - tei: Uses Text Embeddings Inference server
RERANKER_PROVIDER=local
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
# RERANKER_TEI_URL=http://localhost:8081  # For TEI server

# ============================================
# Server Configuration
# ============================================
API_PORT=8888
LOG_LEVEL=info        # Options: debug, info, warning, error, critical
MCP_ENABLED=true      # Enable MCP (Model Context Protocol) server
RUN_MIGRATIONS=true   # Run database migrations on startup

# ============================================
# MCP (Model Context Protocol) Configuration
# ============================================
# MCP allows LLM agents to use retain/recall as tools
MCP_LOCAL_BANK_ID=mcp                    # Default bank ID for MCP operations
# MCP_INSTRUCTIONS=                      # Additional instructions for MCP tool descriptions

# ============================================
# Daemon Mode Configuration
# ============================================
# For running as a background service
DAEMON_PORT=8889       # Port for daemon mode
IDLE_TIMEOUT=0         # Idle timeout in seconds before auto-exit (0 = no auto-exit)

# ============================================
# Performance Tuning
# ============================================
# Database connection pool
DB_POOL_MIN=5
DB_POOL_MAX=20

# Graph retrieval algorithm
# - bfs: Breadth-First Search (default, simpler)
# - mpfp: Multi-Path Flow Propagation (more sophisticated)
GRAPH_RETRIEVER=bfs

# ============================================
# Optimization Flags
# ============================================
# Skip LLM verification on startup (faster startup, no validation)
SKIP_LLM_VERIFICATION=false

# Delay reranker loading until first use (faster startup, slower first query)
LAZY_RERANKER=false
